{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning\n",
    "\n",
    "1. Tokenization\n",
    "2. Stop Word removal\n",
    "3. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the book The Adventures of Sherlock Holmes by Arthur Conan Doyle\n",
    "url = 'http://www.gutenberg.org/ebooks/1661.txt.utf-8'\n",
    "file_name = 'sherlock.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the file and save it locally\n",
    "import urllib.request\n",
    "\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    with open(file_name,'wb') as out_file:\n",
    "        data = response.read()\n",
    "        out_file.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the header and footer infomation. We also don't need the first 33 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE ADVENTURES OF SHERLOCK HOLMES\n"
     ]
    }
   ],
   "source": [
    "#Load the data\n",
    "text = open(file_name, 'r', encoding='utf-8').read()\n",
    "print(text[:33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in the text: 562330\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of characters in the text:\",len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', 'â', 'è', 'é']\n",
      "Number of unique characters: 81\n"
     ]
    }
   ],
   "source": [
    "unique_char = list(set(text))\n",
    "unique_char.sort()\n",
    "print(unique_char)\n",
    "print(\"Number of unique characters:\",len(unique_char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 104493\n"
     ]
    }
   ],
   "source": [
    "#Split by space\n",
    "words = text.split()\n",
    "print(\"Number of words:\",len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N-L-P', 'is', 'an', 'interesting', 'field', 'of', 'AI.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example\n",
    "'N-L-P is an interesting field of AI.'.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to be noted:\n",
    "1. N-L-P might be needed as seperated\n",
    "2. AI. (Punctuation should be removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104493 106000\n"
     ]
    }
   ],
   "source": [
    "#Use Regex to split words\n",
    "import re\n",
    "words_new = re.split('\\W+', text)\n",
    "print(len(words), len(words_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization using spaCy\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[N, -, L, -, P, is, an, interest, field, of, AI]\n"
     ]
    }
   ],
   "source": [
    "words = nlp(\"N-L-P is an interest field of AI\")\n",
    "print([token for token in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 6887\n"
     ]
    }
   ],
   "source": [
    "#Sentence tokenization\n",
    "sentences = list(doc.sents)\n",
    "\n",
    "print(\"Number of sentences:\", len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Word  Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE False False\n",
      "ADVENTURES False False\n",
      "OF False False\n",
      "SHERLOCK False False\n",
      "HOLMES False False\n"
     ]
    }
   ],
   "source": [
    "#Check if a token is a stop word or punctuation\n",
    "for token in doc[:5]:\n",
    "    print(token, token.is_stop, token.is_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert everything to lowercase and then check again\n",
    "text_lower = text.lower()\n",
    "doc_lower = nlp(text_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the True\n",
      "adventures False\n",
      "of True\n",
      "sherlock False\n",
      "holmes False\n"
     ]
    }
   ],
   "source": [
    "for token in doc_lower[:5]:\n",
    "    print(token, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(the, 'the', 7425985699627899538, 'DET'),\n",
       " (adventures, 'adventure', 96151693251643590, 'NOUN'),\n",
       " (of, 'of', 886050111519832510, 'ADP'),\n",
       " (sherlock, 'sherlock', 13444235815076800422, 'NOUN'),\n",
       " (holmes, 'holme', 7908237556874274451, 'NOUN')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token, token.lemma_, token.lemma, token.pos_) for token in doc_lower[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(THE, 'the', 7425985699627899538, 'DET'),\n",
       " (ADVENTURES, 'adventures', 1710777569006724040, 'PROPN'),\n",
       " (OF, 'of', 886050111519832510, 'ADP'),\n",
       " (SHERLOCK, 'sherlock', 13444235815076800422, 'PROPN'),\n",
       " (HOLMES, 'holmes', 406440630240299238, 'PROPN')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token, token.lemma_, token.lemma, token.pos_) for token in doc[:5]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
